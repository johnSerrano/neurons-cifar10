import random
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten, Dropout
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.utils import np_utils
from keras.optimizers import SGD
import numpy as np
import sys
from keras.preprocessing.image import ImageDataGenerator
IMAGE_SIZE = 32
NUM_ITER = 10
BATCH_SIZE = 128
NUM_CATEGORIES = 10


# Extract data from pickle files
def unpickle(file):
    import cPickle
    fo = open(file, 'rb')
    dict = cPickle.load(fo)
    fo.close()
    return dict


# Reshape image data for pyplot
def process_image(img):
    pixels = []
    image = []

    for i in range(IMAGE_SIZE*IMAGE_SIZE):
        pixel = [img[i], img[i+(IMAGE_SIZE*IMAGE_SIZE)],
                img[i+(IMAGE_SIZE*IMAGE_SIZE*2)]]
        pixels += [pixel]

    for i in range(IMAGE_SIZE):
        row = []
        for j in range(IMAGE_SIZE):
            row += [pixels[(IMAGE_SIZE*i)+j]]

    return image


# check command line args for number of iterations
if (len(sys.argv) > 1):
    NUM_ITER = int(sys.argv[1])


# Extract image data from cifar-10 files
print("Processing data...")
data_1 = unpickle("cifar-10-batches-py/data_batch_1")
data_2 = unpickle("cifar-10-batches-py/data_batch_2")
data_3 = unpickle("cifar-10-batches-py/data_batch_3")
data_4 = unpickle("cifar-10-batches-py/data_batch_4")
data_5 = unpickle("cifar-10-batches-py/data_batch_5")
test = unpickle("cifar-10-batches-py/test_batch")
meta = unpickle("cifar-10-batches-py/batches.meta")


#combine datasets
data_all = np.vstack([data_1["data"], data_2["data"], data_3["data"], data_4["data"], data_5["data"]])
labels_all = data_1["labels"] + data_2["labels"] + data_3["labels"] + data_4["labels"] + data_5["labels"]


# Process data for training
test_data = test["data"]
test_labels = np_utils.to_categorical(np.array(test["labels"]), NUM_CATEGORIES)
labels = np_utils.to_categorical(np.array(labels_all), NUM_CATEGORIES)

data_all = data_all.reshape(data_all.shape[0], 3, 32, 32)
test_data = test_data.reshape(test_data.shape[0], 3, 32, 32)

data_all = data_all.astype('float32')
test_data = test_data.astype('float32')
data_all /= 255
test_data /= 255


# Define network
print("Generating model...")
model = Sequential()

model.add(Convolution2D(96, 3, 3, border_mode="same", input_shape=(3, 32, 32)))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(.2))
model.add(Convolution2D(96, 3, 3))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(.2))
model.add(Convolution2D(96, 3, 3, subsample=(2, 2)))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(.3))
model.add(Convolution2D(192, 3, 3))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(.4))
model.add(Convolution2D(192, 3, 3))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(.4))
model.add(Convolution2D(192, 3, 3, subsample=(2, 2)))
model.add(Dropout(.5))
model.add(Flatten())
#model.add(Dense(500, input_dim=3072, init="glorot_uniform"))
#model.add(Activation("relu"))
#model.add(Dropout(.5))
model.add(Dense(10, init="glorot_uniform"))
model.add(Activation("softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adadelta", metrics=["accuracy"])

print(model.summary())

# Train network
print("Training...")
#model.fit(data_all, labels, nb_epoch=NUM_ITER, batch_size=BATCH_SIZE, shuffle=True)

print('Using real-time data augmentation.')

# this will do preprocessing and realtime data augmentation
datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(data_all)

# fit the model on the batches generated by datagen.flow()
model.fit_generator(datagen.flow(data_all, labels,
                    batch_size=BATCH_SIZE),
                    samples_per_epoch=data_all.shape[0],
                    nb_epoch=NUM_ITER,
                        validation_data=(test_data, test_labels))

# Test network
result = model.evaluate(test_data, test_labels, batch_size=BATCH_SIZE, verbose=0, sample_weight=None)
print('Test score:', result[0])
print('Test accuracy:', result[1])


# Predict results for test data
predictions = model.predict_classes(test_data, batch_size=BATCH_SIZE, verbose=0)


# Display 9 random results from test data
test_data = test_data.reshape(test_data.shape[0], 3072)
for i in range(9):
    j = random.randint(0, 10000)
    plt.subplot(3,3,i+1)

    image = process_image(test_data[j])

    plt.imshow(image, cmap='gray', interpolation='none')
    plt.title(meta["label_names"][test["labels"][j]] + " : " + meta["label_names"][predictions[j]])
plt.show()
